{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the command line, create a new environment\n",
    "#conda create --name data_jamboree\n",
    "## Activate the new environment\n",
    "#conda activate data_jamboree\n",
    "## Install packages\n",
    "#conda install ipykernel jupyter ipython matplotlib matplotlib-inline \n",
    "#  notebook numpy  pandas  scikit-learn seaborn scipy statsmodels\n",
    "#conda install -c conda-forge shap\n",
    "#conda install -c plotly plotly\n",
    "#pip install uszipcode geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    " \n",
    "# pandas is a data frame package built \n",
    "# numpy is a numerical computing package\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from numpy.random import choice\n",
    "\n",
    "# plotting libraries \n",
    "import matplotlib.pyplot as plt # python's \"grandfather\" \n",
    "import seaborn as sns # more modern and \"pretty\"\n",
    "import plotly.express as px # interactive plotting\n",
    "\n",
    "# packages for statistical tests \n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.multicomp as mc\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# sklearn is python's most common \"machine learning\" package\n",
    "# It is common to import the specific modules and classes you need\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "# shap is a package for explaining machine learning models\n",
    "import shap\n",
    "\n",
    "# contains info from the US census\n",
    "from uszipcode import SearchEngine\n",
    "\n",
    "# geopy is a package for geocoding and reverse geocoding\n",
    "from geopy.geocoders import GoogleV3, Nominatim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These lines of code show what versions that I used to run this notebook\n",
    "\n",
    "# The ! tell jupyter to execute the command in the terminal (not python)\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modules and versions\n",
    "\n",
    "modules = ['numpy', 'pandas', 'matplotlib', 'seaborn', 'geopy', 'uszipcode', 'sklearn', 'shap']\n",
    "\n",
    "for module in modules:\n",
    "    try:\n",
    "        mod = __import__(module)\n",
    "        version = getattr(mod, '__version__', 'N/A')\n",
    "        print(f\"{module}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"{module}: Not Installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change pandas display options\n",
    "## This will allow us to see more columns when we print a dataframe\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_data = pd.read_csv('nyc311_011523-012123_by022023.csv') # read from local file\n",
    "\n",
    "# GitHub URL \n",
    "url = 'https://github.com/statds/ids-s23/raw/8c68649925d069a1d93a71022b87b26a61b0c180/data/nyc311_011523-012123_by022023.csv'\n",
    "original_data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at shape, tail, and head\n",
    "print(df.shape)\n",
    "#df.head(2)\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  I like to avoid doing math in my head\n",
    "# get the number and pct of missing values in each column\n",
    "df.isna().sum()\n",
    "\n",
    "# fancy - This will show number and percent of missing values for each column\n",
    "# pd.concat([df.isna().sum(), df.isna().mean()], axis=1).rename(columns={0: 'num', 1: 'pct'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1: Fix column names.\n",
    "\n",
    "For ease of comparison across languages, make the column names consistent in style with lowercase using underscore to separate words within a name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Two ways to rename columns\n",
    "\n",
    "# Using rename and a lambda function\n",
    "# df = df.rename(columns=lambda x: x.lower().replace(' ', '_'))\n",
    "\n",
    "# Using a list comprehension to reassign the columns\n",
    "df.columns = [col.lower().replace(' ', '_') for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 2: Check for obvious errors or inefficiencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 2a: For example, are there records whose Closed Date is earlier than or exactly the same as the Created Date?** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all dates to datetime objects \n",
    "# pd.to_datetime() will raise an error by default if there are any invalid dates\n",
    "\n",
    "df['created_date'] = pd.to_datetime(df['created_date'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "df['closed_date'] = pd.to_datetime(df['closed_date'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "df['due_date'] = pd.to_datetime(df['due_date'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "df['resolution_action_updated_date'] = pd.to_datetime(df['resolution_action_updated_date'], format=\"%m/%d/%Y %I:%M:%S %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for closed dates before or equal to created dates\n",
    "errors = df['closed_date'] <= df['created_date']\n",
    "print(errors.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column that indicates if the closed_date is missing (0), if closed date is before/same created date (-1) and 1 otherwise\n",
    "# Missing closed dates are likely still censored (still open)\n",
    "df['closed_date_indicator'] = np.where(df['closed_date'].isna(), 0, np.where(errors, -1, 1))\n",
    "df['closed_date_indicator'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['closed_date'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 2b: Are there invalid values for any columns?** \n",
    "\n",
    "There are a lot of possible variable to consider.  I am going to look at\n",
    "* zip code\n",
    "* latitude/longitude\n",
    "* borough\n",
    "\n",
    "*1. Zip code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check that all zip codes are 5 digits\n",
    "# Convert zip to a string and check the length of the zip codes\n",
    "df['incident_zip'] = df['incident_zip'].astype('Int64').astype('str')\n",
    "zip_length = df['incident_zip'].apply(len)\n",
    "zip_length.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[zip_length==4,'incident_zip'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found a dataset with all the zip codes in NYC [here](https://data.ny.gov/Government-Finance/New-York-State-ZIP-Codes-County-FIPS-Cross-Referen/juva-r6g2/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check that all zip codes are in NYC\n",
    "\n",
    "# read in zip code info (specify that all columns should be strings)\n",
    "all_zips = pd.read_csv('New_York_State_ZIP_Codes-County_FIPS_Cross-Reference.csv', dtype=str)\n",
    "\n",
    "# These are the counties in NYC\n",
    "nyc_counties = ['Bronx', 'Kings', 'New York', 'Queens', 'Richmond']\n",
    "\n",
    "mapping = {'Bronx': 'Bronx',  'Kings': 'Brooklyn', 'New York': 'Manhattan', \n",
    "     'Queens': 'Queens', 'Richmond':'Staten Island'}\n",
    "\n",
    "# df['Category'] = df['Category'].replace(mapping)\n",
    "# print(df)\n",
    "\n",
    "nyc_zips = all_zips.loc[all_zips['County Name'].isin(nyc_counties), ['ZIP Code', 'County Name']]\n",
    "nyc_zips['Borough'] = nyc_zips['County Name'].replace(mapping)\n",
    "\n",
    "# These are the zip codes in NYC\n",
    "zips = nyc_zips['ZIP Code'].unique()\n",
    "zips.sort() # this will change array in place\n",
    "df[~df['incident_zip'].isin(zips)]['incident_zip'].value_counts(dropna=False)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change <NA> to back to a missing value\n",
    "df['incident_zip'].replace('<NA>', np.nan, inplace=True)\n",
    "df['incident_zip'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 12345 is a zip code in Schenectady, NY (about 150 miles from NYC).  It could have been used as a placeholder.\n",
    "* 10977 is a zip code in Sprint Valley, NY (close to NYC, so maybe valid?)\n",
    "* 10000 does not appear to be a real zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_zips = df[~df['incident_zip'].isin(zips)]['incident_zip'].unique()\n",
    "bad_zips = bad_zips[1:]\n",
    "bad_zips_df = df[df['incident_zip'].isin(bad_zips)][['incident_zip', 'latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Later we'll restrict analysis to NYPD, so here I'm just looking at what bad zipcodes are with NYPD\n",
    "# df[df['incident_zip'].isin(bad_zips)][['incident_zip', 'agency']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the plotly function `scatter_mapbox()`, a free token from [mapbox](https://docs.mapbox.com/help/getting-started/access-tokens/) is required.  In this code, my mapbox token is in a file called `.mapbox_token`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If the mapbox token is not set, the map will not display\n",
    "px.set_mapbox_access_token(open(\".mapbox_token\").read())\n",
    "fig = px.scatter_mapbox(bad_zips_df, lon='longitude', lat='latitude', hover_data='incident_zip', zoom=9)\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the `scatter_geo` function also works, but I don't think that it looks as nice for this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter_geo(bad_zips_df, lon='longitude', lat='latitude', \n",
    "#                      hover_data='incident_zip',\n",
    "#                       scope='usa')\n",
    "# fig.update_layout(width=800, height=600)\n",
    "# fig.update_geos(center=dict(lon=df['longitude'].mean(), lat=df['latitude'].mean()),\n",
    "#                 projection_scale=100)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. Latitude and Longitude*\n",
    "\n",
    "(Check to see that all lat/lon values are inside the NYC limits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the access token as been created\n",
    "#px.set_mapbox_access_token(open(\".mapbox_token\").read())\n",
    "fig = px.scatter_mapbox(df, lon='longitude', lat='latitude', color='borough', hover_data='incident_zip',zoom=9)\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter_geo(df, lon='longitude', lat='latitude', color='borough',\n",
    "#                      hover_data='incident_zip', scope='usa')\n",
    "# fig.update_layout(width=800, height=600)\n",
    "# fig.update_geos(center=dict(lon=df['longitude'].mean(), lat=df['latitude'].mean()),\n",
    "#                 projection_scale=50)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3. Borough*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use zip code to get unspecified boroughs\n",
    "\n",
    "# Merge data with zip code/borough info\n",
    "merged_df = df.merge(nyc_zips, left_on='incident_zip', right_on='ZIP Code', how='left')\n",
    "# Fill 'Unspecified' borough values\n",
    "merged_df.loc[merged_df['borough'] == 'Unspecified', 'borough'] = merged_df['Borough'].str.upper()\n",
    "# rename back to \"df\" and drop the columns from the merge\n",
    "df = merged_df.copy().drop(columns=['ZIP Code', 'County Name', 'Borough'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(df, lon='longitude', lat='latitude', color='borough', hover_data='incident_zip',zoom=9)\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2c: Are any columns redundant?**\n",
    "\n",
    "**Redundant Columns**\n",
    "\n",
    "* Location and latitude/longitude contain redundant information\n",
    "* X Coordinate and Y Coordinate \"State Plane\" is another time of coordinate system that is almost perfectly correlated with latitude and longitude for this small area\n",
    "* There is a lot of overlapping information in `incident_address` and the location information in columns `street_name` thru `city`\n",
    "* Agency and Agency name contain the same info\n",
    "* Borough and Park Borough are identical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat/lon correlation with x/y state plane coordinates\n",
    "print(df['latitude'].corr(df['y_coordinate_(state_plane)']))\n",
    "print(df['longitude'].corr(df['x_coordinate_(state_plane)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=df['borough'], columns=df['park_borough'],dropna=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop unnecessary columns if desired\n",
    "df = df.drop(['agency_name','x_coordinate_(state_plane)', 'y_coordinate_(state_plane)',\n",
    "              'incident_address', 'street_name', 'cross_street_1', 'cross_street_2',\n",
    "              'intersection_street_1', 'intersection_street_2', 'address_type', 'city', \n",
    "              'location'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 3: Fill in missing values if possible. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3a: For example, if incident zip code is missing but the location is not, the zip code could be recovered by geocoding.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.incident_zip.isna() & ~df.longitude.isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create variables for filling in missing zip codes\n",
    "# df['zip_fill_nom'] = df['incident_zip']\n",
    "# df['zip_fill_g'] = df['incident_zip']\n",
    "\n",
    "# Geocode with Nominatim\n",
    "#  This takes about 3 minutes to run\n",
    "\n",
    "# def get_zipcode_from_nomin(row):\n",
    "#     if pd.isnull(row['zip_fill_nom']) and not pd.isnull(row['latitude']) and not pd.isnull(row['longitude']):\n",
    "#         geolocator = Nominatim(user_agent=\"data_jamboree\")\n",
    "#         location = geolocator.reverse((row['latitude'], row['longitude']), exactly_one=True)\n",
    "#         address = location.raw.get(\"address\", {})\n",
    "#         return address.get(\"postcode\", None)\n",
    "           \n",
    "#     return row['zip_fill_nom']\n",
    "\n",
    "\n",
    "# df.loc[pd.isnull(df['zip_fill_nom']), 'zip_fill_nom'] = df[pd.isnull(df['zip_fill_nom'])].apply(get_zipcode_from_nomin, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 2 minutes to run\n",
    "\n",
    "# with open('apikey.txt') as f:\n",
    "#     apikey = f.readline().strip()\n",
    "\n",
    "# def get_zipcode_from_google(row):\n",
    "#     if pd.isnull(row['zip_fill_g']) and not pd.isnull(row['latitude']) and not pd.isnull(row['longitude']):\n",
    "#         geolocator = GoogleV3(api_key=apikey)\n",
    "#         location = geolocator.reverse((row['latitude'], row['longitude']), exactly_one=True)\n",
    "#         data = location.raw['address_components']\n",
    "#         postal_code = next((item['short_name'] for item in data if 'postal_code' in item['types']), None)\n",
    "#         return postal_code\n",
    "#     return row['zip_fill_g']  # This line returns the existing value if conditions aren't met.\n",
    "\n",
    "# # Use loc to target rows with missing 'incident_zip' values\n",
    "# df.loc[pd.isnull(df['zip_fill_g']), 'zip_fill_g'] = df[pd.isnull(df['zip_fill_g'])].apply(get_zipcode_from_google, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason, sometimes one method resulted in a \"None\" value while the\n",
    "# other method came back with a valid zipcode.  I am going to combine the \n",
    "# columns so that the missing values of the Google method will be filled with the \n",
    "# missing values of the other method\n",
    "\n",
    "#df['incident_zip'] = df['zip_fill_g'].combine_first(df['zip_fill_nom'])\n",
    "#df.drop(['zip_fill_g', 'zip_fill_nom'], axis=1, inplace=True)\n",
    "#df.to_csv('nyc311_with_geo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nyc311_with_geo.csv', \n",
    "                 parse_dates=['created_date', 'closed_date', 'due_date',\n",
    "                             'resolution_action_updated_date'],\n",
    "                 dtype={'incident_zip': 'str'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Summarize your suggestions to the data curator in several bullet points.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Data manipulation. \n",
    "**Focus only on requests made to NYPD.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nypd = df[df['agency'] == 'NYPD'].copy()\n",
    "nypd.dropna(subset=['incident_zip'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nypd['closed_date_indicator'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nypd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1: Create duration variable.\n",
    "\n",
    "**Create a a new variable duration, which represents the time period from the Created Date to Closed Date. Note that duration may be censored for some requests.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set missing closed dates to the maxiumum closed date\n",
    "nypd.loc[nypd['closed_date'].isna(), 'closed_date'] = nypd['closed_date'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration in hours\n",
    "nypd['duration'] = (nypd['closed_date'] - nypd['created_date']).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 2: Visualize duration\n",
    "\n",
    "**Visualize the distribution of uncensored duration by weekdays/weekend and by borough, and test whether the distributions are the same across weekdays/weekends of their creation and across boroughs.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nypd['created_daytype'] = nypd['created_date'].dt.dayofweek.map(lambda x: 'weekend' if x >= 5 else 'weekday')\n",
    "nypd['created_hour'] = nypd['created_date'].dt.hour\n",
    "uncensored = nypd['closed_date_indicator'] == 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekend vs Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(15,8))\n",
    "sns.boxplot(x='created_daytype', y='duration', data=nypd[uncensored], ax=ax[0])\n",
    "ax[0].set_title('Distribution of Duration by Day Type')\n",
    "\n",
    "sns.boxplot(x='created_daytype', y='duration', showfliers=False,   data=nypd[uncensored], ax=ax[1])\n",
    "ax[1].set_title('Distribution of Duration by Day Type (without outliers)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nypd.groupby('created_daytype')['duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing durations across weekdays/weekends\n",
    "weekday_data = nypd[(nypd['created_daytype'] == 'weekday') & uncensored]['duration']\n",
    "weekend_data = nypd[(nypd['created_daytype'] == 'weekend') & uncensored]['duration']\n",
    "\n",
    "stat, p = ttest_ind(weekday_data, weekend_data)\n",
    "print(f\"Difference in mean: {np.mean(weekday_data) - np.mean(weekend_data)}\")\n",
    "print(f\"p-value for weekday vs weekend duration: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the difference in means\n",
    "def mean_diff(sample1, sample2):\n",
    "    return np.mean(sample1) - np.mean(sample2)\n",
    "\n",
    "# Bootstrap confidence intervals\n",
    "num_bootstraps = 10000\n",
    "alpha = 0.05\n",
    "lower_percentile = 100 * alpha/2\n",
    "upper_percentile = 100 * (1 - alpha/2)\n",
    "\n",
    "# Store the differences\n",
    "bootstrap_diffs = []\n",
    "\n",
    "for _ in range(num_bootstraps):\n",
    "    # Resample each group\n",
    "    boot1 = choice(weekday_data, size=len(weekday_data), replace=True)\n",
    "    boot2 = choice(weekend_data, size=len(weekend_data), replace=True)\n",
    "    \n",
    "    # Compute the difference in medians\n",
    "    diff = mean_diff(boot1, boot2)\n",
    "    bootstrap_diffs.append(diff)\n",
    "\n",
    "# Compute the percentiles for the confidence intervals\n",
    "ci_lower = np.percentile(bootstrap_diffs, lower_percentile)\n",
    "ci_upper = np.percentile(bootstrap_diffs, upper_percentile)\n",
    "\n",
    "print(f\"95% CI for the difference in medians between group1 and group2: [{ci_lower}, {ci_upper}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(15,8))\n",
    "sns.boxplot(x='borough', y='duration', data=nypd[uncensored], ax=ax[0])\n",
    "ax[0].set_title('Distribution of Duration by Borough')\n",
    "\n",
    "sns.boxplot(x='borough', y='duration', showfliers=False,   data=nypd[uncensored], ax=ax[1])\n",
    "ax[1].set_title('Distribution of Duration by Borough (without outliers)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nypd.groupby('borough')['duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing durations across boroughs\n",
    "boroughs = nypd['borough'].dropna().unique()\n",
    "borough_data = [nypd[(nypd['borough'] == borough) & uncensored]['duration'] for borough in boroughs if pd.notnull(borough)]\n",
    "\n",
    "stat, p = f_oneway(*borough_data)\n",
    "print(f\"p-value for durations across boroughs: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the groups into a single data array and a corresponding array of group labels\n",
    "data = nypd.dropna(subset=['duration','borough']).loc[uncensored,'duration']\n",
    "groups = nypd.dropna(subset=['duration','borough']).loc[uncensored,'borough']\n",
    "\n",
    "# Perform the Tukey HSD test\n",
    "tukey_result = mc.pairwise_tukeyhsd(data, groups, alpha=0.05)\n",
    "print(tukey_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 3: Merge with zipcode census data\n",
    "\n",
    "**Basic information at the zipcode level such as population density, median home value, and median household income is available from the US Census. Convenient accesses are, for example, R package zipcodeR and Python package uszipcode; there seems to no Julia equivalent yet but Julia can call R or Python easily. Merge the zipcode level information with the NYPD requests data.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the rows with wrong zip codes\n",
    "nypd = nypd[nypd!='10000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nypd.incident_zip.unique().sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## census data by zip\n",
    "search = SearchEngine()\n",
    "unique_zips = nypd['incident_zip'].dropna().unique().astype('int').astype('str')\n",
    "zip_data = [search.by_zipcode(zipcode) for zipcode in unique_zips]\n",
    "zip_data = pd.DataFrame([z.to_dict() for z in zip_data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(nypd, zip_data, how='left', left_on='incident_zip', right_on='zipcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing lat/lon with values from zipcode dataset\n",
    "merged_df['latitude'] = merged_df['latitude'].fillna(merged_df['lat'])\n",
    "merged_df['longitude'] = merged_df['longitude'].fillna(merged_df['lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that have all missing values\n",
    "merged_df.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Data analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Create the target variable\n",
    "**Define a binary variable over3h which is 1 if duration is greater than 3 hours. Note that it can be obtained even for censored duration.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['target'] = np.where(merged_df['duration'] > 3, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nypd['target'].value_counts()\n",
    "merged_df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude complaint_type for now\n",
    "model_df = merged_df[['complaint_type', 'borough', 'latitude', 'longitude', \n",
    "                     'created_daytype', 'created_hour', 'population', 'population_density', \n",
    "                     'housing_units', 'water_area_in_sqmi', 'occupied_housing_units', \n",
    "                     'median_home_value', 'median_household_income', 'target']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Keep only top 5 categories and combine others into 'other'\n",
    "\n",
    "category_counts = model_df['complaint_type'].value_counts()\n",
    "\n",
    "# Get the 10 least common categories\n",
    "\n",
    "n_combine = len(category_counts) - 5\n",
    "least_common_categories = category_counts.nsmallest(n_combine).index\n",
    "\n",
    "# Replace these categories with 'other'\n",
    "model_df['complaint_type'] = model_df['complaint_type'].replace(least_common_categories, 'other')\n",
    "\n",
    "model_df['complaint_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Few missing values and many are in the same rows \n",
    "## I am going to drop the rows with missing values\n",
    "model_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = model_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_cols.remove('target')\n",
    "cat_cols = model_df.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numeric columns\n",
    "scaler = StandardScaler()\n",
    "model_df[num_cols] = scaler.fit_transform(model_df[num_cols])\n",
    "\n",
    "# Convert categorical columns to dummies\n",
    "model_df = pd.get_dummies(model_df, columns=cat_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the new columns have spaces and special characters that will cause problems later\n",
    "model_df.columns = [col.lower().replace(' ', '_') for col in model_df.columns]\n",
    "model_df.columns = [col.replace('-', '') for col in model_df.columns]\n",
    "model_df.columns = [col.replace('/', '') for col in model_df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_df.drop(['target'], axis=1)\n",
    "y = model_df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Build a logistic model \n",
    "**Build a logistic model to predict over3h using the 311 request data as well as those zip code level covariates. If your model has tuning parameters, justify their choices. Use appropriate metrics to assess the performance of the model.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Using `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'target ~ ' + '+'.join(X.columns)\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.logit(formula=formula, data=model_df)\n",
    "result = model.fit()\n",
    "probabilities = result.predict(model_df)\n",
    "predictions = (probabilities > 0.5).astype(int)\n",
    "\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.accuracy_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.confusion_matrix(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = LogisticRegression(solver='liblinear', scoring='recall', Cs=20)\n",
    "#lr.fit(X,y)\n",
    "lr = LogisticRegression(solver='liblinear', C=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since I am not using a trainig and test set, I am going to use cross validation\n",
    "cross_val_score(lr, X, y, cv=10).mean()\n",
    "#cross_val_score(lr, X, y, scoring='f1', cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(lr, X, y, scoring=['accuracy', 'precision', 'recall', 'f1'], cv=10)\n",
    "for key in cv_scores.keys():\n",
    "    print(f\"{key}: {cv_scores[key].mean():.3f} +/- {cv_scores[key].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.confusion_matrix(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions = cross_val_predict(lr, X, y, cv=10)\n",
    "cv_probabilities = cross_val_predict(lr, X, y, cv=10, method='predict_proba')[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = skm.roc_curve(y, probabilities)\n",
    "roc_auc = skm.auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skm.confusion_matrix(y, cv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skm.classification_report(y, cv_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### These are different from statsmodels because scikit-learn uses regularization by default!\n",
    "lr.fit(X,y)\n",
    "print(lr.intercept_)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Repeat the analysis with another model (e.g., random forest; neural network; etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "cv_scores_rf = cross_validate(rf, X, y, scoring=['accuracy', 'precision', 'recall', 'f1'], cv=10)\n",
    "for key in cv_scores_rf.keys():\n",
    "    print(f\"{key}: {cv_scores_rf[key].mean():.3f} +/- {cv_scores_rf[key].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf = cross_val_predict(rf, X, y)\n",
    "prob_rf = cross_val_predict(rf, X, y, method='predict_proba')[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.confusion_matrix(y, pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skm.classification_report(y, pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = skm.roc_curve(y, prob_rf)\n",
    "roc_auc = skm.auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data=rf.feature_importances_, index=X.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP\n",
    "\n",
    "- **Intuitive Model Interpretations:** SHAP (SHapley Additive exPlanations) provides a unified measure of feature importance for any machine learning model, based on game theory\n",
    "  \n",
    "- **From Global to Local:** Not only does SHAP help in understanding the overall impact of features across the model (global interpretability), but it also dissects individual predictions to show the contribution of each feature (local interpretability)\n",
    "\n",
    "- **Versatility and Adoption:** SHAP is model-agnostic, meaning it can be applied to any machine learning model, from simple linear regressions to complex deep neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the dataset is so large, this took a long time to run (about 3 hours)\n",
    "## Using a testing dataset and/or few features will speed up the process\n",
    "\n",
    "# explainer = shap.TreeExplainer(rf)\n",
    "# explainer.feature_names = features\n",
    "# sv = explainer(X)\n",
    "# shap_values = shap.Explanation(sv[:,:,1], sv.base_values[:,1], X, feature_names=features)\n",
    "\n",
    "# # I saved the results to a pickle file so I don't have to run it again\n",
    "\n",
    "# import pickle\n",
    "# with open('shap_values.pkl', 'wb') as file:\n",
    "#     pickle.dump((shap_values, X), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load it back:\n",
    "import pickle\n",
    "with open('shap_values.pkl', 'rb') as file:\n",
    "    shap_values, X = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
